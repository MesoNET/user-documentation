# Hyperparameter Optimization (HPO) 


L'optimisation d'hyperparamètres (HPO) est une étape cruciale pour
obtenir les meilleures performances d'un modèle d'apprentissage
automatique.
Sur le cluster Turpan, deux outils sont proposés :

-   [**Ray Tune**](./ray_tune.md) : sur ** un seul
    nœud**, jusqu'à **2 GPUs**.
-   [**Optuna**](./optuna.md) : adapté pour **configurations multi-nœuds et
    multi-GPUs**.

------------------------------------------------------------------------
