"use strict";(self.webpackChunkmesodocs=self.webpackChunkmesodocs||[]).push([[342],{4137:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>b});var r=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function s(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?s(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):s(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},s=Object.keys(e);for(r=0;r<s.length;r++)n=s[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(r=0;r<s.length;r++)n=s[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var u=r.createContext({}),l=function(e){var t=r.useContext(u),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},p=function(e){var t=l(e.components);return r.createElement(u.Provider,{value:t},e.children)},c="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},m=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,s=e.originalType,u=e.parentName,p=i(e,["components","mdxType","originalType","parentName"]),c=l(n),m=a,b=c["".concat(u,".").concat(m)]||c[m]||d[m]||s;return n?r.createElement(b,o(o({ref:t},p),{},{components:n})):r.createElement(b,o({ref:t},p))}));function b(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var s=n.length,o=new Array(s);o[0]=m;var i={};for(var u in t)hasOwnProperty.call(t,u)&&(i[u]=t[u]);i.originalType=e,i[c]="string"==typeof e?e:a,o[1]=i;for(var l=2;l<s;l++)o[l]=n[l];return r.createElement.apply(null,o)}return r.createElement.apply(null,n)}m.displayName="MDXCreateElement"},4578:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>u,contentTitle:()=>o,default:()=>d,frontMatter:()=>s,metadata:()=>i,toc:()=>l});var r=n(7462),a=(n(7294),n(4137));const s={title:"SLURM",sidebar_position:10},o=void 0,i={unversionedId:"arch_exp/turpan/Quick_start/slurm",id:"arch_exp/turpan/Quick_start/slurm",title:"SLURM",description:"Slurm (Simple Linux Utility for Resource Management) est un ordonnanceur de t\xe2ches (job scheduler) utilis\xe9 dans les supercalculateurs pour g\xe9rer efficacement l\u2019utilisation des ressources partag\xe9es entre plusieurs utilisateurs.",source:"@site/docs/arch_exp/turpan/Quick_start/slurm.md",sourceDirName:"arch_exp/turpan/Quick_start",slug:"/arch_exp/turpan/Quick_start/slurm",permalink:"/documentation/user-documentation/arch_exp/turpan/Quick_start/slurm",draft:!1,tags:[],version:"current",sidebarPosition:10,frontMatter:{title:"SLURM",sidebar_position:10},sidebar:"tutorialSidebar",previous:{title:"Maqao",permalink:"/documentation/user-documentation/arch_exp/turpan/performance/maqao"},next:{title:"Tutoriaux",permalink:"/documentation/user-documentation/category/tutoriaux"}},u={},l=[{value:"<strong>Mode batch - Script Directives</strong>",id:"mode-batch---script-directives",level:2},{value:"<strong>Variables d&#39;environnement SLURM courantes</strong>",id:"variables-denvironnement-slurm-courantes",level:3},{value:"<strong>Commandes de base SLURM </strong>",id:"commandes-de-base-slurm-",level:3},{value:"<strong>Mode interactifs</strong>",id:"mode-interactifs",level:2},{value:"<strong>Soumettre des jobs avec des d\xe9pendances</strong>",id:"soumettre-des-jobs-avec-des-d\xe9pendances",level:2},{value:"<strong>Automatiser le processus pour plusieurs jobs</strong>",id:"automatiser-le-processus-pour-plusieurs-jobs",level:3}],p={toc:l},c="wrapper";function d(e){let{components:t,...n}=e;return(0,a.kt)(c,(0,r.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("p",null,"Slurm (Simple Linux Utility for Resource Management) est un ordonnanceur de t\xe2ches (job scheduler) utilis\xe9 dans les supercalculateurs pour g\xe9rer efficacement l\u2019utilisation des ressources partag\xe9es entre plusieurs utilisateurs."),(0,a.kt)("p",null,"Il y a deux mani\xe8res de lancer un job Slurm selon ses besoins : en mode batch (script) ou en mode interactif."),(0,a.kt)("h2",{id:"mode-batch---script-directives"},(0,a.kt)("strong",{parentName:"h2"},"Mode batch - Script Directives")),(0,a.kt)("p",null,"C\u2019est la m\xe9thode la plus courante pour soumettre un job. L\u2019utilisateur \xe9crit un script SLURM contenant les instructions et les ressources demand\xe9es, puis le soumet avec sbatch."),(0,a.kt)("p",null,"Cr\xe9ez votre script ",(0,a.kt)("inlineCode",{parentName:"p"},"script.slurm"),". Incluez ces directives au d\xe9but de votre script SLURM (script.slurm). Elles d\xe9finiront les ressources vont \xeatre allou\xe9es, notamment le nombre de c\u0153urs, de n\u0153uds et le temps d'ex\xe9cution. SLURM utilisera ces informations pour g\xe9rer les resource pour votre job."),(0,a.kt)("p",null,"Dans les commentaires (-J,N,n,t,p) , c'est une mani\xe8re courte d'\xe9crire quelques directives."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"#!/bin/bash                         # Shebang obligatoire\n\n# =========================== \n# Directives essentielles de SLURM\n# 4 t\xe2ches par n\u0153ud pour 2 n\u0153uds\n# 4 * 2 = 8 t\xe2ches au total\n# ===========================\n\n#SBATCH --job-name=mon_projet       # Nom du job            (-J JobName)\n#SBATCH --nodes=2                   # Nombre de n\u0153uds       (-N 2)\n#SBATCH --ntasks=8                  # Nombre de t\xe2ches      (-n 1)\n#SBATCH --ntasks-per-node=4         # T\xe2ches par n\u0153ude \n#SBATCH --cpus-per-task=1           # Nombre de c\u0153urs CPU par t\xe2che, threads\n#SBATCH --time=01:00:00             # Limite de temps h:m:s (-t 01:00:00) \n#SBATCH --partition=small           # Nom de la partition   (-p small)\n#SBATCH --gres=gpu:1                # Demander 1 GPU\n#SBATCH --mem=4G                    # M\xe9moire par n\u0153ud\n\n\n# ================================= \n# Directives optionnelles de SLURM \n# ================================= \n\n#SBATCH --mail-user=your_email@domain.com  # O\xf9 envoyer les mails en fonction des \xe9v\xe9nements\n#SBATCH --mail-type=END,FAIL        # \xc9v\xe9nements de mail (NONE, BEGIN, END, FAIL, ALL)\n#SBATCH --output=output_%j.txt      # Sortie standard (%j se remplace par l\u2019ID du job)\n#SBATCH --error=error_%j.txt        # Error log\n#SBATCH --nodelist=node01,node02    # N\u0153ud sp\xe9cifique, node01 (nom du n\u0153ud)\n#SBATCH --exclude=nodename          # Exclure un n\u0153ud\n\n\n# ================================= \n# Commandes d'ex\xe9cution du job\n# ================================= \n# Modules d'environnement : Charger les modules n\xe9cessaires avant d'ex\xe9cuter votre application\n\nmodule load gcc/9.3.0 \nmodule load openmpi/4.0.3\n\n\n# Ex\xe9cuter votre programme\nmpirun -np ${SLURM_NTASKS} ./mon_projet > results_job_${SLURM_JOBID}\n\n")),(0,a.kt)("h3",{id:"variables-denvironnement-slurm-courantes"},(0,a.kt)("strong",{parentName:"h3"},"Variables d'environnement SLURM courantes")),(0,a.kt)("p",null,"SLURM fournit plusieurs variables d'environnement qui peuvent \xeatre utilis\xe9es tout au long du script. Ces variables sont d\xe9finies au d\xe9but du script, ce qui permet de les modifier facilement sans avoir \xe0 mettre \xe0 jour plusieurs valeurs manuellement. Par exemple, la variable ",(0,a.kt)("inlineCode",{parentName:"p"},"SLURM_NTASKS")," contient la valeur sp\xe9cifi\xe9e dans ",(0,a.kt)("inlineCode",{parentName:"p"},"#SBATCH --ntasks"),". Au lieu d'ajuster cette valeur manuellement dans tout le script, il suffit de la modifier en haut du fichier, et ",(0,a.kt)("inlineCode",{parentName:"p"},"SLURM_NTASKS")," sera mis \xe0 jour partout."),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},(0,a.kt)("inlineCode",{parentName:"strong"},"$SLURM_JOBID")),": ID du job"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},(0,a.kt)("inlineCode",{parentName:"strong"},"$SLURM_JOB_NAME")),": Nom du job"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},(0,a.kt)("inlineCode",{parentName:"strong"},"$SLURM_JOB_NODELIST")),": Liste des n\u0153uds allou\xe9s"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},(0,a.kt)("inlineCode",{parentName:"strong"},"$SLURM_NTASKS")),": Nombre total de t\xe2ches"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},(0,a.kt)("inlineCode",{parentName:"strong"},"$SLURM_NTASKS_PER_NODE"))," : Nombre de t\xe2ches par n\u0153ud"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},(0,a.kt)("inlineCode",{parentName:"strong"},"$SLURM_CPUS_PER_TASK")),": Nombre de CPU par t\xe2che (autrement dit, nombre de threads)"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},(0,a.kt)("inlineCode",{parentName:"strong"},"$SLURM_JOB_NODELIST"))," :  Liste des n\u0153uds")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"mpirun -np 8 ./mon_projet  > results_job\n=> mpirun -np ${SLURM_NTASKS} ./mon_projet > results_job_${SLURM_JOBID}\n")),(0,a.kt)("h3",{id:"commandes-de-base-slurm-"},(0,a.kt)("strong",{parentName:"h3"},"Commandes de base SLURM ")),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"Soumettre un job"),": ",(0,a.kt)("inlineCode",{parentName:"li"},"sbatch script.slurm"),", cela lancera le travail et imprimera l'ID"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"V\xe9rifier l'\xe9tat d'un job"),":",(0,a.kt)("inlineCode",{parentName:"li"},"squeue --me")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"Annuler un job"),": ",(0,a.kt)("inlineCode",{parentName:"li"},"scancel <job_id>")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"Voir les d\xe9tails d'un job"),": ",(0,a.kt)("inlineCode",{parentName:"li"},"scontrol show job <job_id>")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"Consulter les informations des n\u0153uds"),": ",(0,a.kt)("inlineCode",{parentName:"li"},"sinfo"))),(0,a.kt)("h2",{id:"mode-interactifs"},(0,a.kt)("strong",{parentName:"h2"},"Mode interactifs")),(0,a.kt)("p",null,"Pour les s\xe9ances interactives :"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"`srun --ntasks=1 --cpus-per-task=4 --mem=8G --time=02:00:00 --partition=shared --pty /bin/bash`\n")),(0,a.kt)("h2",{id:"soumettre-des-jobs-avec-des-d\xe9pendances"},(0,a.kt)("strong",{parentName:"h2"},"Soumettre des jobs avec des d\xe9pendances")),(0,a.kt)("p",null,"Pour chaque job suivant, soumettez-le avec une d\xe9pendance \xe0 l'ID du job pr\xe9c\xe9dent. Cela garantit que chaque job ne d\xe9marre qu'une fois le pr\xe9c\xe9dent termin\xe9 avec succ\xe8s."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"#!/bin/bash\n\n# Soumettre le premier job et capturer son ID\njobid1=$(sbatch --parsable job_script1.slurm)\n\n# Soumettre le deuxi\xe8me job avec une d\xe9pendance au premier job\njobid2=$(sbatch --parsable --dependency=afterok:$jobid1 job_script2.slurm)\n\n# Soumettre le troisi\xe8me job avec une d\xe9pendance au deuxi\xe8me job\njobid3=$(sbatch --parsable --dependency=afterok:$jobid2 job_script3.slurm)\n\n# Continuer ce sch\xe9ma pour les jobs suppl\xe9mentaires selon les besoins\n")),(0,a.kt)("h3",{id:"automatiser-le-processus-pour-plusieurs-jobs"},(0,a.kt)("strong",{parentName:"h3"},"Automatiser le processus pour plusieurs jobs")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},'#!/bin/bash\n\n# Array des scripts de jobs \xe0 soumettre en s\xe9quence\njob_scripts=("job_script1.slurm" "job_script2.slurm" "job_script3.slurm" "job_script4.slurm")\n\n# Initialiser la variable d\'ID du job pr\xe9c\xe9dent\nprev_jobid=""\n\n# Boucler \xe0 travers chaque script de job\nfor script in "${job_scripts[@]}"; do\n    if [ -z "$prev_jobid" ]; then\n        # Soumettre le premier job sans d\xe9pendance\n        prev_jobid=$(sbatch --parsable "$script")\n    else\n        # Soumettre les jobs suivants avec une d\xe9pendance au job pr\xe9c\xe9dent\n        prev_jobid=$(sbatch --parsable --dependency=afterok:$prev_jobid "$script")\n    fi\ndone\n\n')))}d.isMDXComponent=!0}}]);