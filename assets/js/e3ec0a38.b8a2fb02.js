"use strict";(self.webpackChunkmesodocs=self.webpackChunkmesodocs||[]).push([[3361],{4137:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>k});var r=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function s(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?s(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):s(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},s=Object.keys(e);for(r=0;r<s.length;r++)n=s[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(r=0;r<s.length;r++)n=s[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var i=r.createContext({}),u=function(e){var t=r.useContext(i),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},p=function(e){var t=u(e.components);return r.createElement(i.Provider,{value:t},e.children)},c="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},d=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,s=e.originalType,i=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),c=u(n),d=a,k=c["".concat(i,".").concat(d)]||c[d]||m[d]||s;return n?r.createElement(k,o(o({ref:t},p),{},{components:n})):r.createElement(k,o({ref:t},p))}));function k(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var s=n.length,o=new Array(s);o[0]=d;var l={};for(var i in t)hasOwnProperty.call(t,i)&&(l[i]=t[i]);l.originalType=e,l[c]="string"==typeof e?e:a,o[1]=l;for(var u=2;u<s;u++)o[u]=n[u];return r.createElement.apply(null,o)}return r.createElement.apply(null,n)}d.displayName="MDXCreateElement"},8036:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>i,contentTitle:()=>o,default:()=>m,frontMatter:()=>s,metadata:()=>l,toc:()=>u});var r=n(7462),a=(n(7294),n(4137));const s={title:"Lancer un calcul",sidebar_position:3},o="Prise en main rapide de SLURM",l={unversionedId:"code_form/vesta/jobs",id:"code_form/vesta/jobs",title:"Lancer un calcul",description:"Le principe d'un gestionnaire de ressources est de vous r\xe9server des c\u0153urs de calcul en fonction de vos besoins. SLURM est le gestionnaire install\xe9e sur Vesta.",source:"@site/docs/code_form/vesta/jobs.md",sourceDirName:"code_form/vesta",slug:"/code_form/vesta/jobs",permalink:"/documentation/user-documentation/code_form/vesta/jobs",draft:!1,tags:[],version:"current",sidebarPosition:3,frontMatter:{title:"Lancer un calcul",sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"Se connecter \xe0 Vesta",permalink:"/documentation/user-documentation/code_form/vesta/connexion"},next:{title:"Environnements logiciels",permalink:"/documentation/user-documentation/code_form/vesta/module"}},i={},u=[{value:"Les partitions",id:"les-partitions",level:2},{value:"Lancement d&#39;un job",id:"lancement-dun-job",level:2},{value:"Demande de ressources",id:"demande-de-ressources",level:2},{value:"Quelques directives utiles",id:"quelques-directives-utiles",level:2},{value:"Suivre l&#39;\xe9tat d&#39;un job",id:"suivre-l\xe9tat-dun-job",level:2},{value:"Autorisations / Account",id:"autorisations--account",level:2},{value:"Suivre sa consommation",id:"suivre-sa-consommation",level:2},{value:"Espace disque temporaire",id:"espace-disque-temporaire",level:2},{value:"Variables d&#39;environnement",id:"variables-denvironnement",level:2},{value:"Priorit\xe9s",id:"priorit\xe9s",level:2}],p={toc:u},c="wrapper";function m(e){let{components:t,...n}=e;return(0,a.kt)(c,(0,r.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"prise-en-main-rapide-de-slurm"},"Prise en main rapide de SLURM"),(0,a.kt)("p",null,"Le principe d'un gestionnaire de ressources est de vous r\xe9server des c\u0153urs de calcul en fonction de vos besoins. ",(0,a.kt)("strong",{parentName:"p"},"SLURM")," est le gestionnaire install\xe9e sur ",(0,a.kt)("strong",{parentName:"p"},"Vesta"),"."),(0,a.kt)("p",null,"Vous interagissez avec le gestionnaire de ressources par les commandes suivantes :"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"sbatch")," : soumission d'un job dans une file d'attente (appel\xe9es partitions dans Slurm) ;"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"scancel")," : suppression d'un job ;"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"squeue")," : interrogation des jobs ;"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"scontrol")," : interrogation d\xe9taill\xe9e d'un job ou d'une partition ;"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"sinfo")," : interrogation des files d'attente ;"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"srun")," : ex\xe9cution imm\xe9diate d'une commande ;"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"salloc")," : batch interactif, obtention d'un shell, permettant d'encha\xeener plusieurs commandes sur les m\xeames ressources ;"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"sprio")," : priorit\xe9s relatives entre les jobs en attente.")),(0,a.kt)("p",null,"Une documentation des commandes de base est disponible ici : ",(0,a.kt)("a",{parentName:"p",href:"http://slurm.schedmd.com/man_index.html"},"http://slurm.schedmd.com/man_index.html")),(0,a.kt)("h2",{id:"les-partitions"},"Les partitions"),(0,a.kt)("p",null,"Les diff\xe9rents n\u0153uds de calcul sont regroup\xe9s en fonction de diff\xe9rents crit\xe8res dans des partitions.",(0,a.kt)("br",{parentName:"p"}),"\n","Lors de la soumission d'un job, il faut choisir une partition.",(0,a.kt)("br",{parentName:"p"}),"\n","Le cluster Vesta ne poss\xe8de pour le moment qu'une parition nomm\xe9 ",(0,a.kt)("strong",{parentName:"p"},"mesonet")," qui sera utilis\xe9e par d\xe9faut pour tous les jobs. Cette parition poss\xe8de une limite de temps de ",(0,a.kt)("strong",{parentName:"p"},"24 heures")," par job. Cette limite est amen\xe9e \xe0 \xe9voluer au cours du temps."),(0,a.kt)("h2",{id:"lancement-dun-job"},"Lancement d'un job"),(0,a.kt)("p",null,"Pour lancer un job, il faut cr\xe9er un script dans lequel il faut demander des ressources puis appeler son programme (voir les exemples plus loin)."),(0,a.kt)("p",null,"Ce script est ensuite soumi au gestionnaire de file d'attente avec la commande ",(0,a.kt)("strong",{parentName:"p"},"sbatch"),". Par exemple :"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-console"},"sbatch mon_script\n")),(0,a.kt)("p",null,"On obtient alors un num\xe9ro de job, qui peut \xeatre manipul\xe9 avec les commandes scancel, squeue ou scontrol."),(0,a.kt)("p",null,"Le script peut \xeatre \xe9crit dans le langage de son choix (bash, zsh, tcsh, python, perl...). Il peut \xeatre ex\xe9cut\xe9 directement, sans \xeatre appel\xe9 par sbatch, et dans ce cas, les directives d'allocations de ressources seront ignor\xe9es, et il s'ex\xe9cutera dans le shell local."),(0,a.kt)("p",null,"Par d\xe9faut, la sortie standard du job (ce qui doit normalement s'afficher sur l'\xe9cran lorsque vous ex\xe9cutez votre programme) sera \xe9crit dans le fichier ",(0,a.kt)("strong",{parentName:"p"},"slurm-jobid.out"),", avec jobid \xe9gal au num\xe9ro de job."),(0,a.kt)("p",null,"Voici un exemple d'un script de soumission demandant 2 n\u0153uds de calcul avec chacun 10 GPUs :"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-console"},"#! /bin/bash\n\n#SBATCH -p mesonet      # Partition public avec des GPU\n#SBATCH -A account      # Cet account vous sera fourni \xe0 la cr\xe9ation de votre projet\n#SBATCH -N 2-2          # 2 n\u0153ud\n#SBATCH --gres=gpu:10   # 10 GPU par n\u0153ud\n\n./mon_code_gpu\n")),(0,a.kt)("h1",{id:"pour-aller-plus-loin"},"Pour aller plus loin"),(0,a.kt)("h2",{id:"demande-de-ressources"},"Demande de ressources"),(0,a.kt)("p",null,"Vos besoins en terme de ressources sont d\xe9crits dans l'en-t\xeate d'un fichier via des directives Slurm. Par exemple :"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-console"},"#SBATCH -N 2\n")),(0,a.kt)("p",null,"demandera une allocation de 2 n\u0153uds."),(0,a.kt)("p",null,"D'autres crit\xe8res peuvent \xeatre sp\xe9cifi\xe9 via ces directives, comme la taille m\xe9moire souhait\xe9e ou la dur\xe9e pendant laquelle les ressources seront attribu\xe9es."),(0,a.kt)("p",null,"En r\xe8gle g\xe9n\xe9rale, plus on sera ",(0,a.kt)("strong",{parentName:"p"},"parcimonieux dans la demande d'allocation"),", plus on aura de chance de voir rapidement son job passer de l'\xe9tat en attente \xe0 l'\xe9tat en ex\xe9cution."),(0,a.kt)("p",null,"Par exemple, s'il est possible d'estimer pr\xe9cis\xe9ment la dur\xe9e n\xe9cessaire \xe0 une ex\xe9cution, il peut \xeatre profitable de r\xe9duire au minimum la dur\xe9e demand\xe9e pour la r\xe9servation. Ainsi une ex\xe9cution se faisant en 3h30 pourra se faire au sein d'un job demandant 4h00 (marge de 30mn par pr\xe9caution), avec la directive suivante :"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-console"},"#SBATCH -t 04:00:00\n")),(0,a.kt)("p",null,"Attention toutefois \xe0 prendre une marge suffisante, car au-del\xe0 du temps demand\xe9, l'ex\xe9cution est stopp\xe9e automatiquement par Slurm."),(0,a.kt)("p",null,"Attention : toutes les partitions sont configur\xe9es avec une limite de temps d'ex\xe9cution par d\xe9faut, qui s'applique \xe0 tout job ne pr\xe9cisant pas combien de temps doit lui \xeatre allou\xe9. Pour conna\xeetre cette limite, utiliser la commande suivante :"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"sinfo -l\n")),(0,a.kt)("p",null,"Remarque : tous les arguments de la directive #SBATCH peuvent \xe9galement \xeatre utilis\xe9s en arguments des commandes srun, salloc et sbatch. Voir les exemples plus loin."),(0,a.kt)("h2",{id:"quelques-directives-utiles"},"Quelques directives utiles"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"-p")," : partition \xe0 utiliser pour le job"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"-A")," : compte Slurm \xe0 utiliser (il vous est fourni pour chaque projet)"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"-N")," : nombre de n\u0153uds (min - max)"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"-n")," : nombre de t\xe2ches (1 par n\u0153ud par d\xe9faut)"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"--cpus-per-task")," : nombre de c\u0153urs par t\xe2che"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"--tasks-per-node=")," : nombre de t\xe2ches par n\u0153ud"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"-t J-HH:MM:SS")," : temps allou\xe9 du job, avant qu'il ne soit stopp\xe9 (par d\xe9faut, celui de la partition; voir sinfo -l)"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"--gres=gpu:N")," : nombre de GPU par n\u0153ud (N de 1 \xe0 10)"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"--mail-type=END")," : envoyer un mail \xe0 la fin du job (BEGIN pour en recevoir un au lancement. ALL pour en recevoir un \xe0 chaque \xe9tape)"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"--mail-user=mon@adresse")," : adresse mail \xe0 utiliser"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"--mem=0000M")," : m\xe9moire par n\u0153ud en Mo (pour le dire en Go, remplacer M par G)"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"-o fichier_sortie")," : le fichier de sortie du job")),(0,a.kt)("p",null,"Vous trouverez une liste exhaustive sur le site de Slurm : ",(0,a.kt)("a",{parentName:"p",href:"https://slurm.schedmd.com/sbatch.html"},"https://slurm.schedmd.com/sbatch.html")),(0,a.kt)("h2",{id:"suivre-l\xe9tat-dun-job"},"Suivre l'\xe9tat d'un job"),(0,a.kt)("p",null,"Il est possible d'obtenir le d\xe9tail de l'\xe9tat d'un job, qu'il soit en attente ou en ex\xe9cution, avec les commandes ",(0,a.kt)("strong",{parentName:"p"},"scontrol")," ou ",(0,a.kt)("strong",{parentName:"p"},"squeue"),"."),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Liste de tous les jobs en cours :")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-console"},"squeue\n")),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Liste des jobs en cours d'un compte particulier :")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-console"},"squeue -u <login>\n")),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"D\xe9tail de l'\xe9tat d'un job :")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-console"},"scontrol show job <jobid>\n")),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Mon job est en attente. Quand va-t-il d\xe9marrer ?")),(0,a.kt)("p",null,"Pour les jobs en attente, Slurm calcul p\xe9riodiquement un temps probable de d\xe9marrage.",(0,a.kt)("br",{parentName:"p"}),"\n","2 possibilit\xe9s pour l'obtenir:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-console"},"scontrol show job <jobid> | grep StartTime=\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-console"},'squeue -o "%S" -j <jobid>\n')),(0,a.kt)("h2",{id:"autorisations--account"},"Autorisations / Account"),(0,a.kt)("p",null,"Pour utiliser une partition, il faut disposer d'un compte Slurm (ind\xe9pendant du login Unix). Ce compte Slurm est configur\xe9 notamment en fonction du ",(0,a.kt)("strong",{parentName:"p"},"quota d'heures")," attribu\xe9 dans le cadre d'un projet MESONET."),(0,a.kt)("p",null,"Pour utiliser la partition associ\xe9e aux projets, vous aurez acc\xe8s \xe0 un compte Slurm sp\xe9cifique, configur\xe9 avec un quota d'heures suite \xe0 une allocation du comit\xe9 scientifique :"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-console"},"#SBATCH -p mesonet\n#SBATCH -A projet\n")),(0,a.kt)("h2",{id:"suivre-sa-consommation"},"Suivre sa consommation"),(0,a.kt)("p",null,"Vous pouvez avoir une indication de votre consommation en heures CPU par la commande"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-console"},"sreport cluster AccountUtilizationByUser  start=2023-09-01 accounts=votre_account -t hours\n")),(0,a.kt)("p",null,"Notez que ",(0,a.kt)("strong",{parentName:"p"},"votre_account")," correspond au nom du groupe Unix auquel vous appartenez ou alors au num\xe9ro du grant que vous avez obtenu lors d'une demande d'attributions d'heures par le comit\xe9 scientifique."),(0,a.kt)("h2",{id:"espace-disque-temporaire"},"Espace disque temporaire"),(0,a.kt)("p",null,"\xc0 la soumission d\u2019un job et sur le disque dur de chacun des n\u0153uds qui vous sont allou\xe9s, Slurm cr\xe9e un r\xe9pertoire temporaire de nom ",(0,a.kt)("strong",{parentName:"p"},"/scratch/job.$SLURM_JOB_ID")),(0,a.kt)("p",null,"Vous pouvez utiliser ce r\xe9pertoire pour y stocker les fichiers temporaires de vos applications en cours d\u2019ex\xe9cution. Pensez \xe0 les recopier sur votre compte \xe0 la fin du script. En effet, le r\xe9pertoire /scratch/job.$SLURM_JOB_ID est automatiquement effac\xe9 en fin de job."),(0,a.kt)("h2",{id:"variables-denvironnement"},"Variables d'environnement"),(0,a.kt)("p",null,"Dans certains cas, votre programme a besoin de conna\xeetre plus pr\xe9cis\xe9ment les ressources que Slurm a mis \xe0 sa disposition. Pour cela, Slurm fourni un certain nombre de variables d'environnement, qui sont utilisables dans le script qui appelle le programme. En voici une liste non exhaustive :"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"SLURM_NPROCS")," : nombre de c\u0153urs allou\xe9s ;"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"SLURM_NNODES")," : nombre de n\u0153uds allou\xe9s ;"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"SLURM_CPUS_ON_NODE")," : nombre de c\u0153urs allou\xe9s par n\u0153ud ;"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"SLURM_JOB_ID")," : job id ;"),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"SLURM_JOB_NODELIST"),' : liste des n\u0153uds allou\xe9s, sous une forme synth\xe9tique. Pour obtenir une liste d\xe9taill\xe9e, on peut utiliser la commande "',(0,a.kt)("strong",{parentName:"li"},"scontrol show hostname"),'".')),(0,a.kt)("p",null,'Liste d\xe9taill\xe9e : vor chapitre "OUTPUT ENVIRONMENT VARIABLES" sous ',(0,a.kt)("a",{parentName:"p",href:"https://slurm.schedmd.com/sbatch.html"},"https://slurm.schedmd.com/sbatch.html")),(0,a.kt)("h2",{id:"priorit\xe9s"},"Priorit\xe9s"),(0,a.kt)("p",null,"Quand plusieurs jobs sont en m\xeame temps en attente dans une file, Slurm calcule une priorit\xe9 entre ces jobs. Le job ayant la priorit\xe9 la plus \xe9lev\xe9e sera le prochain \xe0 passer en ex\xe9cution."),(0,a.kt)("p",null,"La priorit\xe9 des jobs peut \xeatre vue avec la commande :"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-console"},"sprio -l\n")),(0,a.kt)("p",null,"La priorit\xe9 d\xe9pend de plusieurs facteurs :"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"le temps d'attente d\xe9j\xe0 \xe9coul\xe9 (AGE) ;"),(0,a.kt)("li",{parentName:"ul"},"la taille du job en nombre de c\u0153urs : les gros jobs sont favoris\xe9s (JOBSIZE) ;"),(0,a.kt)("li",{parentName:"ul"},"la consommation en heures cpu sur le pass\xe9 r\xe9cent : plus la consommation a \xe9t\xe9 faible, plus la priorit\xe9 augmentera (FAIRSHARE).")))}m.isMDXComponent=!0}}]);